\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Formatting the Data}{1}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Results}{1}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Methodology}{1}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}General results and comments}{1}{subsection.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Best error in the measurements for respectively GD (Gradient Descent), SGD (Stochastic Gradient Descent), LS (Least Squares), RR (Ridge Regression), LR (Logistic Regression), RLR (Regularized Logistic Regression). The degree field corresponds to the degree of the polynomial basis we used for the prediction, the $\gamma $ is the parameter for every method implying some kind of gradient descent, max\_iters, $\lambda $ is the parameter for every method that implied penalization of the large weights. Note that the results for Ridge Regression were further improved and that our best results are shown on the table \ref  {tab:RR}}}{1}{table.1}}
\newlabel{tab:results}{{I}{1}{Best error in the measurements for respectively GD (Gradient Descent), SGD (Stochastic Gradient Descent), LS (Least Squares), RR (Ridge Regression), LR (Logistic Regression), RLR (Regularized Logistic Regression). The degree field corresponds to the degree of the polynomial basis we used for the prediction, the $\gamma $ is the parameter for every method implying some kind of gradient descent, max\_iters, $\lambda $ is the parameter for every method that implied penalization of the large weights. Note that the results for Ridge Regression were further improved and that our best results are shown on the table \ref {tab:RR}}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Best results : Ridge Regression}{2}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-C}1}Splitting the dataset}{2}{subsubsection.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-C}2}Details of the results}{2}{subsubsection.3.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Expanded results for the best submission with Ridge Regression on a split dataset. The results are the ones with obtained with a cross-validation with several polynomial degrees and $\lambda $ ranging from $10^{-2}$ to $10^{2}$}}{2}{table.2}}
\newlabel{tab:RR}{{II}{2}{Expanded results for the best submission with Ridge Regression on a split dataset. The results are the ones with obtained with a cross-validation with several polynomial degrees and $\lambda $ ranging from $10^{-2}$ to $10^{2}$}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Cross-Validation for the split $0$ for polynomials of degree $10$,$11$ and $\lambda $ ranging from $10^{-3}$ to $10^3$.}}{2}{figure.1}}
\newlabel{fig:cross-validation}{{1}{2}{Cross-Validation for the split $0$ for polynomials of degree $10$,$11$ and $\lambda $ ranging from $10^{-3}$ to $10^3$}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Conclusion}{2}{section.4}}
