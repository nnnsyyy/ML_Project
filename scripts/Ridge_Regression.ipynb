{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementatation of ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let us turn to the proper machine learning side of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run ridge regression function here computes everything we need in our problem. It works the follwing way.\n",
    "- We define the parameters for our run, basically the degrees and the lambdas we want.\n",
    "- The cross_validation function performs a k-fold cross-validation (takes a lot of time) and then returns the best degree of polynomial along with the associated lambda. \n",
    "- Then we just have to re-run the ridge regression once more with those parameters to get the optimal weights.\n",
    "- The final step is to run the model on the testing data set, do our prediction and save the result\n",
    "\n",
    "VERY IMPORTANT NOTE :\n",
    "- The sanitation and standardization of the data are part of our modelling process, there should hence be included into the cross-validation process, as they differ for each sample that we consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from costs import compute_mse\n",
    "from build_polynomial import build_poly\n",
    "from plots import cross_validation_visualization\n",
    "from helpers import build_k_indices\n",
    "from helpers import sanitize_NaN\n",
    "from helpers import standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    return np.linalg.solve(np.dot(tx.T,tx)+lamb*np.identity(tx.shape[1]),np.dot(tx.T,y))#/(2*len(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(y,tX,degrees,lambdas,k_fold,seed):\n",
    "    \"\"\"\n",
    "        Computes the cross_validation for the given parameters and returns the best result for each polynomial degree.\n",
    "        Note that we give the RAW data to the cross_validation, without any transformation on them.\n",
    "    \"\"\"\n",
    "\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # cross validation:    \n",
    "    rmse_best = np.zeros(len(degrees))\n",
    "    rmse_best_lambda = np.zeros(len(degrees))\n",
    "    for j,degree in enumerate(degrees):\n",
    "        \n",
    "        print('\\n Testing for a polynomial of degree ', degree)\n",
    "        #Training and testing errors for each lambda, so we are able to visualize them afterwards.\n",
    "        rmse_tr = np.zeros(len(lambdas))\n",
    "        rmse_te = np.zeros(len(lambdas))\n",
    "        \n",
    "        for i,lambda_ in enumerate(lambdas):\n",
    "            print('lambda=',round(lambda_,6),end=\", \")\n",
    "            \n",
    "            #This is actually where the k-fold cross-validation is computed. We sum all the errors and then average them. \n",
    "            loss_tr_tot=0\n",
    "            loss_te_tot=0\n",
    "            for k in range(k_fold+1):\n",
    "                loss_tr_tmp,loss_te_tmp =cross_validation_rr(y,tX,k_indices,k,lambda_,degree)\n",
    "                loss_tr_tot += loss_tr_tmp\n",
    "                loss_te_tot += loss_te_tmp\n",
    "                \n",
    "            rmse_tr[i] = loss_tr_tot/k_fold\n",
    "            rmse_te[i] = loss_te_tot/k_fold\n",
    "            print('RMSE_BEST_VALUE : ',rmse_te[i])\n",
    "        rmse_best[j] = min(rmse_te)\n",
    "        rmse_best_lambda[j] = lambdas[int(np.argmin(rmse_te))]\n",
    "        cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "        \n",
    "    print('\\nBest error :',rmse_best)\n",
    "    print('Best lambda :',rmse_best_lambda)\n",
    "    return rmse_best,rmse_best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def cross_validation_rr(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression for each step of the k-fold cross validation.\"\"\"\n",
    "    \n",
    "    # get k'th subgroup in test, others in train: \n",
    "    x_test = np.array(x[k_indices[k-1]])\n",
    "    y_test = np.array(y[k_indices[k-1]])\n",
    "    x_train = np.empty((0,x.shape[1]))\n",
    "    y_train =  np.empty((0,1))\n",
    "    #This for loops gets the other groups\n",
    "    for k_iter,validation_points in enumerate(k_indices):\n",
    "        if(k_iter!=k-1):\n",
    "            x_train=np.append(x_train,x[validation_points],axis=0)\n",
    "            y_train=np.append(y_train,y[validation_points])\n",
    "    #we sanitize and standardize our training data here, and apply the same median, mean and variance to the testing data  \n",
    "    x_train,median_train = sanitize_NaN(x_train)\n",
    "    x_test,median_test = sanitize_NaN(x_test,median_train)\n",
    "    \n",
    "    x_train,mean_tr,std_tr = standardize(x_train)\n",
    "    x_test, mean_te,ste_te = standardize(x_test,mean_tr,std_tr)\n",
    "    \n",
    "    # form data with polynomial degree:\n",
    "    x_train_poly = build_poly(x_train,degree)\n",
    "    x_test_poly = build_poly(x_test,degree)\n",
    "\n",
    "    # ridge regression: \n",
    "    w_rr = ridge_regression(y_train,x_train_poly,lambda_)\n",
    "    \n",
    "    # calculate the loss for train and test data:\n",
    "    #loss_tr = sum(abs(y_train-predict_labels(w_rr,x_train_poly)))/len(y_train)\n",
    "    #loss_te = sum(abs(y_test-predict_labels(w_rr,x_test_poly)))/len(y_test)\n",
    "    loss_tr = np.sqrt(2*compute_mse(y_train,x_train_poly,w_rr))\n",
    "    loss_te = np.sqrt(2*compute_mse(y_test,x_test_poly,w_rr))\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from ridge_regression import cross_validation, ridge_regression\n",
    "from helpers import standardize\n",
    "from helpers import sanitize_NaN\n",
    "def run_ridge_regression_sanitized(y, tX):\n",
    "    \"\"\"ridge regression running script. works on the RAW data\"\"\"\n",
    "    #Let us first clean the input\n",
    "    tX,median_tr = sanitize_NaN(tX)\n",
    "    #tX,mean_tr,std_tr = standardize(tX)\n",
    "    \n",
    "    # define parameters for our run   \n",
    "    seed = 1\n",
    "    degrees = np.array([3])\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-3,2,5)\n",
    "    \n",
    "    rmse,lambda_ = cross_validation(y,tX,degrees,lambdas,k_fold,seed)\n",
    "    \n",
    "    weights = ridge_regression(y, tX, lambda_[0])\n",
    "\n",
    "    DATA_TEST_PATH = 'data/test.csv'  # Download train data and supply path here \n",
    "    y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "    tX_test_sorted = sanitize_NaN(tX_test,median_tr)\n",
    "    \n",
    "    OUTPUT_PATH = 'data/output_sanitized_normalization_test.csv' # Fill in desired name of output file for submission\n",
    "    y_pred = predict_labels(weights, tX_test_sorted)\n",
    "    create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "\n",
    "run_ridge_regression_sanitized(y,tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : \n",
    "- train les data avec la mediane et variance et moyenne qu'on calcul√©es avant.\n",
    "- faire pareil pour la cross_validation"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
