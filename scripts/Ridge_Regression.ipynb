{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementatation of ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluding at the moment all the columns that contains NaN (Data set to zero or -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 19)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exclude_NaN(tX):\n",
    "    sort_no_NaN= [1,2,3,7,8,9,10,11,13,14,15,16,17,18,19,20,21,28,29]\n",
    "    tX_reduced = tX[:,sort_no_NaN]\n",
    "    return tX_reduced\n",
    "tX = exclude_NaN(tX)\n",
    "tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization of the data, to get 0 mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def normalize(tX):\n",
    "    return (tX-np.mean(tX,axis=0))/np.std(tX,axis=0)\n",
    "\n",
    "tX = normalize(tX)\n",
    "\n",
    "#We verify whether our data have really mean 0 and variance 1\n",
    "print(np.mean(tX,axis=0))\n",
    "print(np.std(tX,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let us turn to the proper machine learning side of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that builds a polynomial basis from each of the data entry (i.e. goes from $\\mathbb{R}^N$ to $\\mathbb{R}^{N\\cdot D + 1}$ for each data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def new_build_poly(x, degree):\n",
    "    \"\"\"polynomial basis function.\"\"\"\n",
    "    # Creates the matrix with the degrees we want to apply to our data x\n",
    "    # replecating the vector with [1 2 3 4 ... degree] to a matrix\n",
    "    #degree_mat = np.tile(list(range(0,degree+1)), (len(x), 1))\n",
    "    #return np.transpose(np.power(x,np.transpose(degree_mat)))\n",
    "    \n",
    "    # Only 1 time 1, because we don't need it to be repeated. That's why for M features and a polynomial of degree D,\n",
    "    # we will get M*D+1 entries.\n",
    "    X=np.zeros((x.shape[0],(degree)*x.shape[1]+1))\n",
    "    for i in range(1,degree+1):\n",
    "        for j in range(x.shape[1]):\n",
    "            #print((i-1)*(x.shape[1])+j+1)\n",
    "            X[:,(i-1)*x.shape[1]+j+1]=x[:,j]**i\n",
    "    X[:,0]=1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing 4-fold cross-validation in the following two methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from costs import compute_mse\n",
    "from ridge_regression import ridge_regression\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    # get k'th subgroup in test, others in train: \n",
    "    # ***************************************************\n",
    "    x_train = np.array(x[k_indices[k-1]])\n",
    "    y_train = np.array(y[k_indices[k-1]])\n",
    "    x_test = np.empty((0,x.shape[1]))\n",
    "    y_test =  np.empty((0,1))\n",
    "    for k_iter,validation_points in enumerate(k_indices):\n",
    "        if(k_iter!=k-1):\n",
    "            x_test=np.append(x_test,x[validation_points],axis=0)\n",
    "            y_test=np.append(y_test,y[validation_points])\n",
    "    #print(x_test.shape)\n",
    "    #print(y_test.shape)\n",
    "    # ***************************************************\n",
    "    # form data with polynomial degree:\n",
    "    # ***************************************************\n",
    "    x_train_poly = new_build_poly(x_train,degree)\n",
    "    x_test_poly = new_build_poly(x_test,degree)\n",
    "    # ***************************************************\n",
    "    # ridge regression: \n",
    "    # ***************************************************\n",
    "    w_rr = ridge_regression(y_train,x_train_poly,lambda_)\n",
    "    # ***************************************************\n",
    "    # calculate the loss for train and test data:\n",
    "    # ***************************************************\n",
    "    #print(x_train_poly.dot(w_rr).shape)\n",
    "    #print(y_train.shape)\n",
    "    loss_tr = np.sqrt(2*compute_mse(y_train,x_train_poly,w_rr))\n",
    "    loss_te = np.sqrt(2*compute_mse(y_test,x_test_poly,w_rr))\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    return np.linalg.solve(np.dot(tx.T,tx)+lamb*np.identity(tx.shape[1]),np.dot(tx.T,y))#/(2*len(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_ridge_regression(y, tX):\n",
    "    \"\"\"ridge regression running script.\"\"\"\n",
    "    \n",
    "    # define parameters for our run   \n",
    "    seed = 6\n",
    "    degree = 2\n",
    "    split_ratio = 0.5\n",
    "    lambdas = np.logspace(-4, 2, 20)   \n",
    "    weights_arr = np.zeros((len(lambdas),(degree)*tX.shape[1]+1))\n",
    "    errors = []\n",
    "    # ***************************************************\n",
    "    # train on the whole data and use polynomial basis functions\n",
    "    # ***************************************************\n",
    "    tX_train = build_poly(tX, degree)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # ridge regression with different lambdas: \n",
    "    # ***************************************************\n",
    "    for i,lambd in enumerate(lambdas):\n",
    "        #print(weights_arr[i,:].shape, ' ',X_train.shape)\n",
    "        weights_arr[i,:] = ridge_regression(y,tX_train,lambd)\n",
    "        \n",
    "        #rmse_tr[i] = np.sqrt(2*co.compute_loss(y_train,X_train,weights_arr[i,:]))\n",
    "        #rmse_te[i] = np.sqrt(2*co.compute_loss(y_test,X_test,weights_arr[i,:]))\n",
    "                \n",
    "        #print(\"lambda ={l}, proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "        #        l=lambd,p=ratio, d=degree, tr=rmse_tr[i], te=rmse_te[i]))\n",
    "    return weights_arr, errors\n",
    "\n",
    "weights_arr, errors = run_ridge_regression(y, tX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing for a polynomial of degree  1\n",
      "lambda=0.0001, 0.000133, 0.000176, 0.000233, 0.000309, 0.000409, 0.000543, 0.00072, 0.000954, 0.001265, 0.001677, 0.002223, 0.002947, 0.003907, 0.005179, 0.006866, 0.009103, 0.012068, 0.015999, 0.02121, 0.028118, 0.037276, 0.049417, 0.065513, 0.086851, 0.11514, 0.152642, 0.202359, 0.26827, 0.355648, 0.471487, 0.625055, 0.828643, 1.098541, 1.456348, 1.930698, 2.559548, 3.393222, 4.498433, 5.963623, 7.906043, 10.481131, 13.894955, 18.4207, 24.420531, 32.374575, 42.919343, 56.89866, 75.431201, 100.0, \n",
      " Testing for a polynomial of degree  2\n",
      "lambda=0.0001, 0.000133, 0.000176, 0.000233, 0.000309, 0.000409, 0.000543, 0.00072, 0.000954, 0.001265, 0.001677, 0.002223, 0.002947, 0.003907, 0.005179, 0.006866, 0.009103, 0.012068, 0.015999, 0.02121, 0.028118, 0.037276, 0.049417, 0.065513, 0.086851, 0.11514, 0.152642, 0.202359, 0.26827, 0.355648, "
     ]
    }
   ],
   "source": [
    "from plots import cross_validation_visualization \n",
    "\n",
    "def cross_validation_rr():\n",
    "    seed = 20\n",
    "    #degrees = range(2,11)\n",
    "    degrees = np.array([1,2])\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 2, 50)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    # ***************************************************\n",
    "    # cross validation:\n",
    "    # ***************************************************    \n",
    "    rmse_best = np.zeros(len(degrees))\n",
    "    rmse_best_lambda = np.zeros(len(degrees))\n",
    "    for j,degree in enumerate(degrees):\n",
    "        print('\\n Testing for a polynomial of degree ', degree)\n",
    "        rmse_tr = np.zeros(len(lambdas))\n",
    "        rmse_te = np.zeros(len(lambdas))\n",
    "        print('lambda=',end=\"\")\n",
    "        for i,lambda_ in enumerate(lambdas):\n",
    "            print(round(lambda_,6),end=\", \")\n",
    "            loss_tr_tot=0\n",
    "            loss_te_tot=0\n",
    "            for k in range(k_fold+1):\n",
    "                loss_tr_tmp,loss_te_tmp =cross_validation(y,tX,k_indices,k,lambda_,degree)\n",
    "                loss_tr_tot += loss_tr_tmp\n",
    "                loss_te_tot += loss_te_tmp\n",
    "            rmse_tr[i] = loss_tr_tot/k_fold\n",
    "            rmse_te[i] = loss_te_tot/k_fold\n",
    "        rmse_best[j] = min(rmse_te)\n",
    "        rmse_best_lambda[j] = lambdas[int(np.argmin(rmse_te))]\n",
    "        cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    print('\\n',rmse_best_index)\n",
    "    print(rmse_best_lambda)\n",
    "    return rmse_best,rmse_best_lambda\n",
    "    #plt.figure()\n",
    "    #plt.plot(degrees,rmse_best)\n",
    "\n",
    "rmse,lambda_ = cross_validation_rr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lamdba_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-55b078570496>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlamdba_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lamdba_' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
