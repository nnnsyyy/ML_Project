{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementatation of ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING THE DATA:  DONE\n",
      "SPLITTING THE DATA:  DONE\n",
      "\n",
      "\tCROSS VALIDATION FOR SPLIT NUMBER 0\n",
      "\n",
      " Testing for a polynomial of degree  11\n",
      "lambda= 0.01, Percentage of classification error :  0.271959324205\n",
      "lambda= 100.0, Percentage of classification error :  0.20180759068\n",
      "\n",
      "Best degree : 11\n",
      "Best error : 0.20180759068\n",
      "Best lambda : 100.0\n",
      "Size of the vectors (99913,) (99913, 210)\n",
      "\n",
      "\tCROSS VALIDATION FOR SPLIT NUMBER 1\n",
      "\n",
      " Testing for a polynomial of degree  11\n",
      "lambda= 0.01, Percentage of classification error :  0.244364489838\n",
      "lambda= 100.0, Percentage of classification error :  0.243784174146\n",
      "\n",
      "Best degree : 11\n",
      "Best error : 0.243784174146\n",
      "Best lambda : 100.0\n",
      "Size of the vectors (77544,) (77544, 254)\n",
      "\n",
      "\tCROSS VALIDATION FOR SPLIT NUMBER 2\n",
      "\n",
      " Testing for a polynomial of degree  11\n",
      "lambda= 0.01, Percentage of classification error :  0.209504525965\n",
      "lambda= 100.0, Percentage of classification error :  0.209206765126\n",
      "\n",
      "Best degree : 11\n",
      "Best error : 0.209206765126\n",
      "Best lambda : 100.0\n",
      "Size of the vectors (50379,) (50379, 331)\n",
      "\n",
      "\tCROSS VALIDATION FOR SPLIT NUMBER 3\n",
      "\n",
      " Testing for a polynomial of degree  11\n",
      "lambda= 0.01, Percentage of classification error :  0.209213138423\n",
      "lambda= 100.0, Percentage of classification error :  0.210972748601\n",
      "\n",
      "Best degree : 11\n",
      "Best error : 0.209213138423\n",
      "Best lambda : 0.01\n",
      "Size of the vectors (22164,) (22164, 331)\n",
      "Degrees [11, 11, 11, 11]\n",
      "Lambdas [100.0, 100.0, 100.0, 0.01]\n",
      "\n",
      "IMPORTING TESTING DATA : DONE\n",
      "SPLITTING TESTING DATA : DONE\n",
      "PREDICTION FOR TESTING DATA SPLIT NUMBER 0\n",
      "Size of the vectors (227458,) (227458, 30)\n",
      "Counting NaN ."
     ]
    }
   ],
   "source": [
    "import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import *\n",
    "from ridge_regression import *\n",
    "from helpers import *\n",
    "#5. TEST THE MODEL AND EXPORT THE RESULTS\n",
    "DATA_TEST_PATH = '../data/test.csv'  # Download train data and supply path here \n",
    "print('\\nIMPORTING TESTING DATA :',end=\" \")\n",
    "y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print('DONE')\n",
    "    \n",
    "#5.a. Splitting the testing data\n",
    "print('SPLITTING TESTING DATA :',end=\" \")\n",
    "y_test_split,tx_test_split,id_test_split = split_dataset(y_test,tX_test,ids_test)    \n",
    "print('DONE')    \n",
    "#5.b. prediction on each model\n",
    "y_pred = list()\n",
    "    \n",
    "for split,(y_test_s,tx_test_s,id_test_s) in enumerate(zip(y_test_split,tx_test_split,id_test_split)):  \n",
    "    print('PREDICTION FOR TESTING DATA SPLIT NUMBER',split)\n",
    "        \n",
    "    #Formatting to the correct datatype\n",
    "    y_test_s = np.squeeze(y_test_s)\n",
    "    tx_test_s = np.squeeze(tx_test_s)\n",
    "    id_test_s = np.squeeze(id_test_s)\n",
    "    print('Size of the vectors',y_test_s.shape,tx_test_s.shape) \n",
    "    #Formatting the data themselves\n",
    "    print('Counting NaN',end=' .')\n",
    "    tx_test_s = count_NaN(tx_test_s)\n",
    "    print('Sanitizing',end = ' .')\n",
    "    tx_test_s,median_vec = sanitize_NaN(tx_test_s,median_split[split])\n",
    "    print('Standardizing',end = ' .')\n",
    "    tx_test_s,mean_te,std_te = standardize(tx_test_s,mean_split[split],std_split[split])\n",
    "    print('Building polynomial basis',end = ' .')        \n",
    "    tx_test_s = build_poly(tx_test_s, degrees_split[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_NaN(tX):\n",
    "    \"\"\"\n",
    "        Counts the number of NaNs in a column and adds it in a column at the end in order for the information to be conserved.\n",
    "        @param tX : the input of our data.\n",
    "        \n",
    "    \"\"\"\n",
    "    negative_NaN_table = np.array([0,4,5,6,12,23,24,25,26,27,28])\n",
    "    NEGATIVE_NAN = -999.0\n",
    "    zero_NaN_table = [29]\n",
    "    ZERO_NAN = 0\n",
    "    nan_count = np.zeros((tX.shape[0]))\n",
    "    for i in range (tX.shape[0]):\n",
    "            nan_count[i]=nan_count[i]+(tX[i,negative_NaN_table]== NEGATIVE_NAN).sum()+(tX[i,zero_NaN_table] == ZERO_NAN)\n",
    "    return np.c_[tX, nan_count]\n",
    "test= count_NaN(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   7.,   8., ...,   7.,  11.,  12.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249999\n",
      "250000\n"
     ]
    }
   ],
   "source": [
    "split_0 = np.squeeze(np.where(tX[:,22]==0))\n",
    "print(split_0[-1])\n",
    "print(len(y))\n",
    "y_0 = [y[i] for i in split_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([100003, 100004, 100008, ..., 349996, 349998, 349999])], [array([100001, 100002, 100007, ..., 349989, 349991, 349997])], [array([100000, 100006, 100023, ..., 349980, 349985, 349994])], [array([100005, 100011, 100031, ..., 349966, 349992, 349993])]]\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(y,tX,ids):\n",
    "    \"\"\"\n",
    "        Splits the initial dataset into four smaller datasets, according to the the PRI_jet_num\n",
    "        We do the splits manually because we ended having problems with the datatypes when we tried to do\n",
    "        something more automatised.\n",
    "        @param y : the raw y of our data\n",
    "        @param tX : the raw features matrix from our data\n",
    "        @param ids : the ids for the features of our data (splitting them will allow us merge them correctly later on)\n",
    "        @return : 3 tuples with 4 arrays each containing the splitted version of our data\n",
    "    \"\"\"\n",
    "    split_0 = np.where(tX[:,22]==0)\n",
    "    split_1 = np.where(tX[:,22]==1)\n",
    "    split_2 = np.where(tX[:,22]==2)\n",
    "    split_3 = np.where(tX[:,22]==3)\n",
    "    \n",
    "    y_0 = [y[i] for i in split_0]\n",
    "    y_1 = [y[i] for i in split_1]\n",
    "    y_2 = [y[i] for i in split_2]\n",
    "    y_3 = [y[i] for i in split_3]\n",
    "    \n",
    "    ids0 =  [ids[i] for i in split_0]\n",
    "    ids1 =  [ids[i] for i in split_1]\n",
    "    ids2 =  [ids[i] for i in split_2]\n",
    "    ids3 =  [ids[i] for i in split_3]\n",
    "    \n",
    "    return [y_0,y_1,y_2,y_3],[tX[split_0,:],tX[split_1,:],tX[split_2,:],tX[split_3,:]],[ids0,ids1,ids2,ids3]\n",
    "y_split,tx_split,id_split = split_dataset(y,tX,ids)\n",
    "#print(y_split)\n",
    "print(id_split)\n",
    "#test = np.squeeze(t[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[3, 2, 6, 7] [ 1  5 10 11]\n",
      "1\n",
      "[32, 65, 76, 87] [0 2 6 7]\n",
      "2\n",
      "[12, 12, 12, 12] [ 3  8 12 13]\n",
      "3\n",
      "[1, 1, 1, 1] [ 4  9 14 15]\n"
     ]
    }
   ],
   "source": [
    "y1 = [[3,2,6,7],[32,65,76,87],[12,12,12,12],[1,1,1,1]]\n",
    "ids1 = [np.array([1,5,10,11]),np.array([0,2,6,7]),np.array([3,8,12,13]),np.array([4,9,14,15])]\n",
    "\n",
    "for index,(y_s,id_s) in enumerate(zip(y1,ids1)):\n",
    "    print(index)\n",
    "    print(y_s,id_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  2  6  7 32 65 76 87 12 12 12 12  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "y_new = np.hstack(y1)\n",
    "ids_new =np.hstack(id_split)\n",
    "print(y_new)\n",
    "#[(ids_out,y_out) for (ids_out,y_out) in sorted(zip(ids_new,y_new))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge_dataset(y_split,id_split):\n",
    "    \"\"\"\n",
    "        Given the y_split tuple and id_split tuple, merges them and sorts them according to the id\n",
    "        @param y_split : tuple with 4 entries, containing our predictions from the model.\n",
    "        @param id_split : the id of each of our predictions\n",
    "        @return : the merged list of predictions y and the merged ids (sorted w.r.t to ids)\n",
    "    \"\"\"\n",
    "    y_tot = np.squeeze(np.hstack(y_split))\n",
    "    ids_tot = np.squeeze(np.hstack(id_split))\n",
    "    y_id_merged = np.array([(y_out,ids_out) for (ids_out,y_out) in sorted(zip(ids_tot,y_tot))])\n",
    "    return y_id_merged[:,0],y_id_merged[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 100000.  100001.  100002. ...,  349997.  349998.  349999.]\n"
     ]
    }
   ],
   "source": [
    "y_m,ids_m = merge_dataset(y_split,id_split)\n",
    "print(ids_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.   2.   6.   7.  32.  65.  76.  87.  12.  12.  12.  12.   1.   1.   1.\n",
      "   1.]\n",
      "[  1.   5.  10.  11.   0.   2.   6.   7.   3.   8.  12.  13.   4.   9.  14.\n",
      "  15.]\n"
     ]
    }
   ],
   "source": [
    "ids_t = np.empty(0)\n",
    "ids_t = np.append(ids_t,ids1[0])\n",
    "ids_t = np.append(ids_t,ids1[1])\n",
    "ids_t = np.append(ids_t,ids1[2])\n",
    "ids_t = np.append(ids_t,ids1[3])\n",
    "\n",
    "y_t = np.empty(0)\n",
    "y_t = np.append(y_t,y1[0])\n",
    "y_t = np.append(y_t,y1[1])\n",
    "y_t = np.append(y_t,y1[2])\n",
    "y_t = np.append(y_t,y1[3])\n",
    "\n",
    "print(y_t)\n",
    "print(ids_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 32.0),\n",
       " (1.0, 3.0),\n",
       " (2.0, 65.0),\n",
       " (3.0, 12.0),\n",
       " (4.0, 1.0),\n",
       " (5.0, 2.0),\n",
       " (6.0, 76.0),\n",
       " (7.0, 87.0),\n",
       " (8.0, 12.0),\n",
       " (9.0, 1.0),\n",
       " (10.0, 6.0),\n",
       " (11.0, 7.0),\n",
       " (12.0, 12.0),\n",
       " (13.0, 12.0),\n",
       " (14.0, 1.0),\n",
       " (15.0, 1.0)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x_out,y_out) for (x_out,y_out) in sorted(zip(ids_t,y_t))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99913, 30)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_dataset(y_test_split,ids_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x, mean_x=None, std_x=None):\n",
    "    \"\"\"\n",
    "        Standardize the original data set. \n",
    "        If the standard deviation of a column is 0, we remove it.\n",
    "        @param x : the input 2d array that we want to standardize\n",
    "        @param mean_x : the mean we want to apply to the columns\n",
    "        @param std_x : the standard deviation we want to apply to the columns\n",
    "        @return tx : the standardized input x with the columns with std == 0 removed\n",
    "        @return mean_x : the means of the columns (including those which are removed later on)\n",
    "        @return std_x : the standard deviations of the columns (including those which are removed later on)\n",
    "    \"\"\"\n",
    "    if mean_x is None:\n",
    "        mean_x = np.mean(x, axis=0)\n",
    "        \n",
    "    x = x - mean_x\n",
    "    \n",
    "    if std_x is None:\n",
    "        std_x = np.std(x, axis=0)\n",
    "    #Iterate over all columns of the table. If its std is 0, we remove it from the dataset, it means that\n",
    "    #all its values are the same\n",
    "    excluded_col = np.empty((1,0))    \n",
    "    for i in range(x.shape[1]):\n",
    "        if std_x[i] == 0:\n",
    "            excluded_col = np.append(excluded_col, i)\n",
    "        else:\n",
    "            x[:, i] = x[:, i] / std_x[i]\n",
    "    tx = np.array(x)\n",
    "    tx = np.delete(tx, excluded_col, axis=1)\n",
    "    return tx, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.471404520791\n",
      "1.24721912892\n",
      "(array([[-0.70710678, -1.06904497],\n",
      "       [ 1.41421356,  1.33630621],\n",
      "       [-0.70710678, -0.26726124]]), array([ 1.33333333,  3.        ,  3.33333333,  0.        ]), array([ 0.47140452,  0.        ,  1.24721913,  0.        ]))\n"
     ]
    }
   ],
   "source": [
    "x_test = [[1,3,2,0],[2,3,5,0],[1,3,3,0]]\n",
    "print(standardize(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_NaN(tX):\n",
    "    \"\"\"\n",
    "        Counts the number of NaNs in a column and adds it in a column at the end in order for the information to be conserved.\n",
    "        @param tX : the input of our data.\n",
    "        @return the concatenation of tX and its newest column\n",
    "        \n",
    "    \"\"\"\n",
    "    negative_NaN_table = np.array([0,4,5,6,12,23,24,25,26,27,28])\n",
    "    NEGATIVE_NAN = -999.0\n",
    "    zero_NaN_table = [29]\n",
    "    ZERO_NAN = 0\n",
    "    nan_count = np.zeros((tX.shape[0]))\n",
    "    for i in range (tX.shape[0]):\n",
    "        for index,col in enumerate(negative_NaN_table):\n",
    "            nan_count[i]=nan_count[i]+(tX[i,col]== NEGATIVE_NAN)\n",
    "        for index,col in enumerate(zero_NaN_table):\n",
    "            nan_count[i]=nan_count[i]+(tX[i,col] == ZERO_NAN)\n",
    "    return np.c_[tX, nan_count]\n",
    "\n",
    "def sanitize_NaN(tX,median_vec=None):\n",
    "    \"\"\"\n",
    "    Removes the NaNs from the data and replace it with the median of the valid data. \n",
    "    The columns are hard coded, represent the columns from the dataset for the project 1\n",
    "    Returns the computed median, and can apply a median taken as input. \n",
    "    The input median has to be the median of the NaN columns below.\n",
    "    @param tX : the raw data from train.csv\n",
    "    @param median_vec : the vector of the medians that were previously returned from this function \n",
    "                        (contains the median of the valid input of the columns of the array below.)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = tX.copy()\n",
    "    #Hard coding of the columns of the data from train.csv that contains some NaNs in their columns.\n",
    "    #There are two types of NaNs, either -999 or 0, and we distinguish both cases \n",
    "    #(our vector median_vec does not, it simply contains all the medians of the valid data)\n",
    "    negative_NaN_table = np.array([0,4,5,6,12,23,24,25,26,27,28])\n",
    "    NEGATIVE_NAN = -999.0\n",
    "    zero_NaN_table = [29]\n",
    "    ZERO_NAN = 0\n",
    "    # Compute the median of the valid data is no median is provided\n",
    "    if median_vec is None:\n",
    "        n_iter=0\n",
    "        median_vec = np.zeros(len(negative_NaN_table) + len(zero_NaN_table))\n",
    "        for row in negative_NaN_table:\n",
    "            x_without_nan = x[:,row][np.where(x[:,row] != NEGATIVE_NAN)]\n",
    "            #We need to distinguish the case where we have only NaNs in the column, which happens when we\n",
    "            #split the data with our split_dataset method.\n",
    "            if len(x_without_nan > 0):\n",
    "                median_vec[n_iter] = np.median(x_without_nan)\n",
    "            else:\n",
    "                median_vec[n_iter] = 0\n",
    "            n_iter=n_iter+1\n",
    "        for row in zero_NaN_table:\n",
    "            x_without_nan = x[:,row][np.where(x[:,row] != ZERO_NAN)]\n",
    "            #We also distinguish the columns here.\n",
    "            if len(x_without_nan > 0):\n",
    "                median_vec[n_iter] = np.median(x_without_nan)\n",
    "            else: \n",
    "                median_vec[n_iter] = 0\n",
    "            n_iter=n_iter+1\n",
    "    else:\n",
    "        assert len(median_vec) == len(negative_NaN_table) + len(zero_NaN_table)\n",
    "        \n",
    "    #Replace the NaN values with the median of the table        \n",
    "    for i,row in enumerate(negative_NaN_table):\n",
    "        x[:,row][np.where(x[:,row] == NEGATIVE_NAN)] = median_vec[i]\n",
    "    for j,row in enumerate(zero_NaN_table):\n",
    "        x[:,row][np.where(x[:,row] == ZERO_NAN)] = median_vec[i+j+1]\n",
    "    return x, median_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(99913, 30)\n",
      "0 (99913, 19)\n",
      "(77544, 30)\n",
      "1 (77544, 23)\n",
      "(50379, 30)\n",
      "2 (50379, 30)\n",
      "(22164, 30)\n",
      "3 (22164, 30)\n"
     ]
    }
   ],
   "source": [
    "split_data = split_dataset(tX)\n",
    "print(tX.shape)\n",
    "for index,split in enumerate(split_data):\n",
    "    split = np.squeeze(split)\n",
    "    print(split.shape)\n",
    "    split = count_NaN(split)\n",
    "    split,median = sanitize_NaN(split)\n",
    "    split,mean,std = standardize(split)\n",
    "    print(index,split.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [1,3,4,5]\n",
    "x2 = [3,4,5,54,55,4]\n",
    "x3=[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec = list()\n",
    "vec.append(x)\n",
    "vec.append(x2)\n",
    "vec.append(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [3, 4, 5, 54, 55, 4], [1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[1,-1,1,-1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 0.0\n"
     ]
    }
   ],
   "source": [
    "test = np.squeeze(split_data[2])\n",
    "print(np.mean(test[:,22]),np.std(test[:,22]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.empty((0,4))\n",
    "a = np.array([[1,2,3,4]])\n",
    "b = np.array([[3,2,1,4]])\n",
    "x = np.vstack((x,a,b))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(t[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_NaN(tX):\n",
    "    \"\"\"\n",
    "        Counts the number of NaNs in a column and adds it in a column at the end in order for the information to be conserved.\n",
    "        @param tX : the input of our data.\n",
    "        \n",
    "    \"\"\"\n",
    "    negative_NaN_table = np.array([0,4,5,6,12,23,24,25,26,27,28])\n",
    "    NEGATIVE_NAN = -999.0\n",
    "    zero_NaN_table = [29]\n",
    "    ZERO_NAN = 0\n",
    "    nan_count = np.zeros((tX.shape[0]))\n",
    "    for i in range (tX.shape[0]):\n",
    "        for index,col in enumerate(negative_NaN_table):\n",
    "            #print(tX[i,col]==)\n",
    "            nan_count[i]=nan_count[i]+(tX[i,col]== NEGATIVE_NAN)\n",
    "        for index,col in enumerate(zero_NaN_table):\n",
    "            nan_count[i]=nan_count[i]+(tX[i,col] == ZERO_NAN)\n",
    "    return np.c_[tX, nan_count]\n",
    "#print(count_NaN(tX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from array import array\n",
    "float_array = [3333333333333333333333.14, 2.7, 0.0, -1.0, 1.1]\n",
    "np.savetxt('test',float_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.33333333e+21   2.70000000e+00   0.00000000e+00  -1.00000000e+00\n",
      "   1.10000000e+00]\n"
     ]
    }
   ],
   "source": [
    "weights = np.loadtxt('test') \n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let us turn to the proper machine learning side of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run ridge regression function here computes everything we need in our problem. It works the follwing way.\n",
    "- We define the parameters for our run, basically the degrees and the lambdas we want.\n",
    "- The cross_validation function performs a k-fold cross-validation (takes a lot of time) and then returns the best degree of polynomial along with the associated lambda. \n",
    "- Then we just have to re-run the ridge regression once more with those parameters to get the optimal weights.\n",
    "- The final step is to run the model on the testing data set, do our prediction and save the result\n",
    "\n",
    "VERY IMPORTANT NOTE :\n",
    "- The sanitation and standardization of the data are part of our modelling process, there should hence be included into the cross-validation process, as they differ for each sample that we consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ridge_regression import cross_validation, ridge_regression\n",
    "def run_ridge_regression(y, tX):\n",
    "    \"\"\"ridge regression running script. works on the RAW data\"\"\"\n",
    "    \n",
    "    # define parameters for our run   \n",
    "    seed = 12\n",
    "    \n",
    "    #not possible yet to run polynomial  degrees at the same time.\n",
    "    degrees = np.array([3])\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-1,2,50)\n",
    "    \n",
    "    rmse,lambda_ = cross_validation(y,tX,degrees,lambdas,k_fold,seed)\n",
    "    #Let us now clean the input\n",
    "    tX,median_tr = sanitize_NaN(tX)\n",
    "    tX,mean_tr,std_tr = standardize(tX)\n",
    "    tX = build_poly(tX,degrees[0])\n",
    "    \n",
    "    weights = ridge_regression(y, tX, lambda_[0])\n",
    "\n",
    "    print('Weights on whole set\\n',weights)\n",
    "    \n",
    "    DATA_TEST_PATH = 'data/test.csv'  # Download train data and supply path here \n",
    "    y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "    \n",
    "    tX_test_sorted,median_vec = sanitize_NaN(tX_test,median_tr)\n",
    "    tX_test_sorted,mean_tr,std_tr = standardize(tX_test_sorted,mean_tr,std_tr)\n",
    "    tX_test_sorted = build_poly(tX_test_sorted, degrees[0])\n",
    "    OUTPUT_PATH = 'data/output_sanitized_normalization_degree3_lambda_finer.csv' # Fill in desired name of output file for submission\n",
    "    y_pred = predict_labels(np.array(weights), np.array(tX_test_sorted))\n",
    "    create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "\n",
    "run_ridge_regression_sanitized(y,tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import *\n",
    "from ridge_regression import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def run():\n",
    "    \"\"\"ridge regression running script. works on the RAW data\"\"\"\n",
    "    \n",
    "    #0. DEFINE PARAMETERS FOR OUR RUN\n",
    "    seed = 1\n",
    "    \n",
    "    #not possible yet to run polynomial  degrees at the same time.\n",
    "    degrees = np.array([3])\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-1,2,50)\n",
    "    \n",
    "    #1. LOAD THE DATA\n",
    "    print('LOADING THE DATA: ',end=\" \")\n",
    "    DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "    y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "    print('DONE')\n",
    "    \n",
    "    #2. RUN CROSS VALIDATION TO GET BEST LAMBDA\n",
    "    print('CROSS VALIDATION')\n",
    "    rmse,lambda_ = cross_validation(y,tX,degrees,lambdas,k_fold,seed)\n",
    "    #Let us now clean the input\n",
    "    tX,median_tr = sanitize_NaN(tX)\n",
    "    tX,mean_tr,std_tr = standardize(tX)\n",
    "    tX = build_poly(tX,degrees[0])\n",
    "    \n",
    "    #3. TRAIN THE MODEL\n",
    "    weights = ridge_regression(y, tX, lambda_[0])\n",
    "\n",
    "    print('Weights on whole set\\n',weights)\n",
    "    \n",
    "    #4. TEST THE MODEL AND EXPORT THE RESULTS\n",
    "    DATA_TEST_PATH = 'data/test.csv'  # Download train data and supply path here \n",
    "    print('IMPORTING TESTING DATA :',end=\" \")\n",
    "    y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "    print('DONE')\n",
    "    \n",
    "    tX_test_sorted,median_vec = sanitize_NaN(tX_test,median_tr)\n",
    "    tX_test_sorted,mean_tr,std_tr = standardize(tX_test_sorted,mean_tr,std_tr)\n",
    "    tX_test_sorted = build_poly(tX_test_sorted, degrees[0])\n",
    "    OUTPUT_PATH = 'data/output_sanitized_normalization_degree1_lambda_finer_test.csv' # Fill in desired name of output file for submission\n",
    "    print('EXPORTING TESTING DATA WITH PREDICTIONS :',end=\" \")\n",
    "    y_pred = predict_labels(np.array(weights), np.array(tX_test_sorted))\n",
    "    create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "    print('DONE')\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTING TESTING DATA : DONE\n"
     ]
    }
   ],
   "source": [
    "    DATA_TEST_PATH = '../data/test.csv'  # Download train data and supply path here \n",
    "    print('IMPORTING TESTING DATA :',end=\" \")\n",
    "    y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227458, 30)\n"
     ]
    }
   ],
   "source": [
    "y_split,tx_split,id_split = split_dataset(y_test,tX_test,ids_test)\n",
    "print(np.squeeze(tx_split[0]).shape)\n",
    "for split,(y_s,tx_s,id_s) in enumerate(zip(y_split,tx_split,id_split)):\n",
    "    tx_s = np.squeeze(tx_s)    \n",
    "    tx_test = count_NaN(tx_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227458, 31)\n"
     ]
    }
   ],
   "source": [
    "print(tx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLITTING TESTING DATA : DONE\n",
      "PREDICTION FOR TESTING DATA SPLIT NUMBER 0\n",
      "Size of the vectors (227458,) (227458, 30)\n",
      "Counting NaN .PREDICTION FOR TESTING DATA SPLIT NUMBER 1\n",
      "Size of the vectors (175338,) (175338, 30)\n",
      "Counting NaN .PREDICTION FOR TESTING DATA SPLIT NUMBER 2\n",
      "Size of the vectors (114648,) (114648, 30)\n",
      "Counting NaN .PREDICTION FOR TESTING DATA SPLIT NUMBER 3\n",
      "Size of the vectors (50794,) (50794, 30)\n",
      "Counting NaN ."
     ]
    }
   ],
   "source": [
    "print('SPLITTING TESTING DATA :',end=\" \")\n",
    "y_test_split,tx_test_split,id_test_split = split_dataset(y_test,tX_test,ids_test)    \n",
    "print('DONE')    \n",
    "#5.b. prediction on each model\n",
    "y_pred = list()\n",
    "\n",
    "for split,(y_test_s,tx_test_s,id_test_s) in enumerate(zip(y_test_split,tx_test_split,id_test_split)):  \n",
    "    print('PREDICTION FOR TESTING DATA SPLIT NUMBER',split)\n",
    "        \n",
    "    #Formatting to the correct datatype\n",
    "    y_test_s = np.squeeze(y_test_s)\n",
    "    tx_test_s = np.squeeze(tx_test_s)\n",
    "    id_test_s = np.squeeze(id_test_s)\n",
    "    print('Size of the vectors',y_test_s.shape,tx_test_s.shape) \n",
    "    #Formatting the data themselves\n",
    "    print('Counting NaN',end=' .')\n",
    "    tx_test_new = count_NaN(tx_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING THE DATA:  DONE\n",
      "SPLITTING THE DATA:  DONE\n",
      "\n",
      "\tCROSS VALIDATION FOR SPLIT NUMBER 0\n",
      "\n",
      " Testing for a polynomial of degree  11\n",
      "lambda= 0.01, Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Percentage of classification error :  0.271959324205\n",
      "lambda= 100.0, Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Percentage of classification error :  0.20180759068\n",
      "\n",
      "Best degree : 11\n",
      "Best error : 0.20180759068\n",
      "Best lambda : 100.0\n",
      "Concatenating array DONE\n",
      "Size of the vectors (99913,) (99913, 210)\n",
      "\n",
      "\tCROSS VALIDATION FOR SPLIT NUMBER 1\n",
      "\n",
      " Testing for a polynomial of degree  11\n",
      "lambda= 0.01, Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Percentage of classification error :  0.244364489838\n",
      "lambda= 100.0, Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Percentage of classification error :  0.243784174146\n",
      "\n",
      "Best degree : 11\n",
      "Best error : 0.243784174146\n",
      "Best lambda : 100.0\n",
      "Concatenating array DONE\n",
      "Size of the vectors (77544,) (77544, 254)\n",
      "\n",
      "\tCROSS VALIDATION FOR SPLIT NUMBER 2\n",
      "\n",
      " Testing for a polynomial of degree  11\n",
      "lambda= 0.01, Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Percentage of classification error :  0.209504525965\n",
      "lambda= 100.0, Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Percentage of classification error :  0.209206765126\n",
      "\n",
      "Best degree : 11\n",
      "Best error : 0.209206765126\n",
      "Best lambda : 100.0\n",
      "Concatenating array DONE\n",
      "Size of the vectors (50379,) (50379, 331)\n",
      "\n",
      "\tCROSS VALIDATION FOR SPLIT NUMBER 3\n",
      "\n",
      " Testing for a polynomial of degree  11\n",
      "lambda= 0.01, Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Percentage of classification error :  0.209213138423\n",
      "lambda= 100.0, Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Concatenating array DONE\n",
      "Percentage of classification error :  0.210972748601\n",
      "\n",
      "Best degree : 11\n",
      "Best error : 0.209213138423\n",
      "Best lambda : 0.01\n",
      "Concatenating array DONE\n",
      "Size of the vectors (22164,) (22164, 331)\n",
      "Degrees [11, 11, 11, 11]\n",
      "Lambdas [100.0, 100.0, 100.0, 0.01]\n",
      "\n",
      "IMPORTING TESTING DATA : DONE\n",
      "SPLITTING TESTING DATA : DONE\n",
      "PREDICTION FOR TESTING DATA SPLIT NUMBER 0\n",
      "Size of the vectors (227458,) (227458, 30)\n",
      "Counting NaN ."
     ]
    }
   ],
   "source": [
    "import run.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMPORTING TESTING DATA : DONE\n",
      "SPLITTING TESTING DATA : DONE\n",
      "PREDICTION FOR TESTING DATA SPLIT NUMBER 0\n",
      "Size of the vectors (227458,) (227458, 30)\n",
      "Counting NaN .Sanitizing ."
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'median_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ba5a7dc6e391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtx_test_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_NaN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx_test_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Sanitizing'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' .'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtx_test_s\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmedian_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msanitize_NaN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx_test_s\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmedian_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Standardizing'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' .'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtx_test_s\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstd_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx_test_s\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstd_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'median_split' is not defined"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "#5. TEST THE MODEL AND EXPORT THE RESULTS\n",
    "DATA_TEST_PATH = '../data/test.csv'  # Download train data and supply path here \n",
    "print('\\nIMPORTING TESTING DATA :',end=\" \")\n",
    "y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print('DONE')\n",
    "    \n",
    "#5.a. Splitting the testing data\n",
    "print('SPLITTING TESTING DATA :',end=\" \")\n",
    "y_test_split,tx_test_split,id_test_split = split_dataset(y_test,tX_test,ids_test)    \n",
    "print('DONE')    \n",
    "#5.b. prediction on each model\n",
    "y_pred = list()\n",
    "    \n",
    "for split,(y_test_s,tx_test_s,id_test_s) in enumerate(zip(y_test_split,tx_test_split,id_test_split)):  \n",
    "    print('PREDICTION FOR TESTING DATA SPLIT NUMBER',split)\n",
    "        \n",
    "    #Formatting to the correct datatype\n",
    "    y_test_s = np.squeeze(y_test_s)\n",
    "    tx_test_s = np.squeeze(tx_test_s)\n",
    "    id_test_s = np.squeeze(id_test_s)\n",
    "    print('Size of the vectors',y_test_s.shape,tx_test_s.shape) \n",
    "    #Formatting the data themselves\n",
    "    print('Counting NaN',end=' .')\n",
    "    tx_test_s = count_NaN(tx_test_s)\n",
    "    print('Sanitizing',end = ' .')\n",
    "    tx_test_s,median_vec = sanitize_NaN(tx_test_s,median_split[split])\n",
    "    print('Standardizing',end = ' .')\n",
    "    tx_test_s,mean_te,std_te = standardize(tx_test_s,mean_split[split],std_split[split])\n",
    "    print('Building polynomial basis',end = ' .')        \n",
    "    tx_test_s = build_poly(tx_test_s, degrees_split[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
